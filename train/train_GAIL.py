import os
import argparse
import torch
import torch.nn as nn
import logging
import numpy as np
from argparse import Namespace

from imitation.data.types import TrajectoryWithRew
from stable_baselines3.common.vec_env import VecEnvWrapper
from imitation.data import rollout
from imitation.rewards.reward_wrapper import RewardVecEnvWrapper
from imitation.util.util import make_vec_env

from imitation.rewards.reward_nets import BasicRewardNet
from stable_baselines3.common.torch_layers import MlpExtractor

from imitation.data.wrappers import RolloutInfoWrapper

from imitation.algorithms.adversarial.gail import GAIL
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from imitation.data.rollout import flatten_trajectories
from adapter.adapter_shoot_back import SB3SingleCombatEnv
from net.net_shoot_imitation import CustomImitationPolicy, MLPBase, GRULayer
from stable_baselines3.common.callbacks import BaseCallback


# æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ç½‘ç»œï¼ˆç›´æ¥è¾“å‡ºè¿ç»­åŠ¨ä½œï¼‰
class ImitationPolicy(nn.Module):
    def __init__(self, obs_dim):
        super().__init__()
        self.feature_extractor = MLPBase(obs_dim, 128)
        self.gru = GRULayer(128, 128)
        self.action_head = nn.Linear(128, 5)  # è¿ç»­åŠ¨ä½œè¾“å‡º 5ç»´

    def forward(self, x):
        feat = self.feature_extractor(x)
        feat = self.gru(feat)
        raw = self.action_head(feat)
        # å‰3ç»´ tanh [-1, 1]ï¼Œç¬¬4ç»´ sigmoid * 0.5 + 0.4 => [0.4, 0.9]ï¼Œç¬¬5ç»´ sigmoid => [0,1]
        aileron = torch.tanh(raw[:, 0:1])
        elevator = torch.tanh(raw[:, 1:2])
        rudder = torch.tanh(raw[:, 2:3])
        throttle = torch.sigmoid(raw[:, 3:4]) * 0.5 + 0.4
        shoot = torch.sigmoid(raw[:, 4:5])
        return torch.cat([aileron, elevator, rudder, throttle, shoot], dim=-1)


class Freezer:
    def __init__(self, policy, num_old_logits=11, unfreeze_step=50000):
        self.policy = policy
        self.num_old_logits = num_old_logits
        self.unfreeze_step = unfreeze_step
        self.unfrozen = False
        self._freeze_feature_extractor()
        print("âœ… ç‰¹å¾æå–å™¨å·²å†»ç»“")

    def _freeze_feature_extractor(self):
        for param in self.policy.features_extractor.parameters():
            param.requires_grad = False

    def filter_gradients(self, current_step):
        """æ‰‹åŠ¨æ¸…é™¤å‰ä¸‰ç»´ action è¾“å‡ºå±‚çš„æ¢¯åº¦"""
        if current_step <= self.unfreeze_step:
            action_weight = self.policy.action_net.weight
            action_bias = self.policy.action_net.bias

            if action_weight.grad is not None:
                action_weight.grad[:self.num_old_logits] = 0.0
            if action_bias.grad is not None:
                action_bias.grad[:self.num_old_logits] = 0.0

    def maybe_unfreeze(self, current_step):
        if not self.unfrozen and current_step >= self.unfreeze_step:
            for param in self.policy.parameters():
                param.requires_grad = True
            self.unfrozen = True
            print("ğŸ”“ å·²è§£å†»å…¨éƒ¨å‚æ•°")


def gail_custom_callback(rollout_result):
    global last_save_step
    step = gail_trainer.gen_algo.num_timesteps

    # å†»ç»“æ§åˆ¶
    freezer.maybe_unfreeze(step)
    freezer.filter_gradients(step)

    # # è·å–æœ€è¿‘ rollout ä¸­çš„è§‚æµ‹å€¼
    # obs = rollout_result.observations
    # obs_tensor = torch.as_tensor(obs, device=ppo.device)
    #
    # # æ‰‹åŠ¨ forward ç­–ç•¥ï¼Œè·å– logits
    # with torch.no_grad():
    #     features = ppo.policy.extract_features(obs_tensor)
    #     latent_pi, _ = ppo.policy.mlp_extractor(features)
    #     logits = ppo.policy.action_net(latent_pi)
    #     if logits.shape[1] > 12:
    #         fire_logit = logits[:, 12].mean().item()
    #         ppo.logger.record("custom/fire_logit", fire_logit)

    # âœ… æ¯ X æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    save_interval = 100_000
    if step - last_save_step >= save_interval:
        save_path = f"./trained_model/shoot_back/check_point/gail_ppo_checkpoint_{step}.zip"
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        ppo.save(save_path)
        last_save_step = step
        print(f"ğŸ“¦ æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{save_path}")


class EnvIDFilter(logging.Filter):
    def __init__(self, env_id):
        super().__init__()
        self.env_id = env_id

    def filter(self, record):
        record.env_id = f"{self.env_id}"
        return True


def setup_logging(env_id=0, log_file=None):
    log_dir = os.path.dirname(log_file)
    os.makedirs(log_dir, exist_ok=True)
    """é…ç½® loggingï¼Œè®©æ—¥å¿—æ—¢è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œåˆå†™å…¥æ–‡ä»¶ï¼Œæ ‡æ˜ ENV ID"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    # åˆ›å»º Filterï¼Œç”¨äºæ³¨å…¥ env_id
    env_filter = EnvIDFilter(env_id)

    # æ—¥å¿—æ ¼å¼å¸¦ env_id
    formatter = logging.Formatter("%(asctime)s - %(levelname)s [ENV %(env_id)s] - %(message)s")

    # ç»ˆç«¯ handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    console_handler.addFilter(env_filter)

    # æ–‡ä»¶ handler
    file_handler = logging.FileHandler(log_file, mode="a")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    file_handler.addFilter(env_filter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    logging.info(f"Logger for ENV {env_id} initialized, log path: {log_file}")


def make_env(env_id, config_name, log_file):
    def _init():
        setup_logging(env_id, log_file)
        env = SB3SingleCombatEnv(
            env_id,
            config_name=config_name,
        )
        return env
    return _init


def load_expert_trajectories(npz_path, max_episode_len=None):
    data = np.load(npz_path)
    observations = data["observations"]
    actions = data["actions"]
    rewards = data["rewards"]  # âœ… åŠ è½½ reward

    if max_episode_len is None:
        max_episode_len = 200

    trajectories = []
    total_steps = observations.shape[0]
    num_trajs = total_steps // max_episode_len

    for i in range(num_trajs):
        start = i * max_episode_len
        end = (i + 1) * max_episode_len

        traj_obs = observations[start:end+1]
        traj_acts = actions[start:end]
        traj_rews = rewards[start:end]  # âœ… çœŸå® reward ä¼ è¿›æ¥

        traj = TrajectoryWithRew(
            obs=traj_obs,
            acts=traj_acts,
            rews=traj_rews,
            infos=None,
            terminal=True,
        )
        trajectories.append(traj)

    print(f"âœ… æˆåŠŸåŠ è½½ {len(trajectories)} æ¡ expert trajectory")
    return trajectories


class HybridRewardWrapper(VecEnvWrapper):
    def __init__(self, venv, gail_reward_fn, alpha=0.9, beta=0.1):
        super().__init__(venv)
        self.gail_reward_fn = gail_reward_fn
        self.alpha = alpha
        self.beta = beta

    def step_wait(self):
        obs, env_rewards, dones, infos = self.venv.step_wait()
        acts = [info["act"] for info in infos]  # æå‰åœ¨ rollout æ—¶è®°å½• act

        # è·å– GAIL reward
        gail_rewards = self.gail_reward_fn(obs, acts, None)

        # æ··åˆ reward
        mixed_rewards = self.alpha * gail_rewards + self.beta * env_rewards
        return obs, mixed_rewards, dones, infos


# 4. è®¾ç½®å¥–åŠ±æ··åˆæœºåˆ¶
class MixedRewardEnv(RewardVecEnvWrapper):
    def __init__(self, venv, reward_fn, imitation_weight=0.9, env_weight=0.1):
        super().__init__(venv, reward_fn)
        self.imitation_weight = imitation_weight
        self.env_weight = env_weight
        self.last_obs = None
        self.last_act = None

    def reset(self):
        obs = self.venv.reset()
        self.last_obs = obs
        return obs

    def step_async(self, actions):
        self.last_act = actions
        return self.venv.step_async(actions)

    def step_wait(self):
        obs, env_rewards, dones, infos = self.venv.step_wait()
        # imitation_rewards ä½¿ç”¨å½“å‰ obs, action, next_obs, done
        imitation_rewards = self.reward_fn(
            self.last_obs, self.last_act, obs, dones
        )
        mixed_rewards = self.imitation_weight * imitation_rewards + self.env_weight * env_rewards
        self.last_obs = obs
        return obs, mixed_rewards, dones, infos



class ActRecorderWrapper(RolloutInfoWrapper):
    def step_wait(self):
        obs, rewards, dones, infos = super().step_wait()
        actions = self.actions
        for i, info in enumerate(infos):
            info["act"] = actions[i]
        return obs, rewards, dones, infos


# âœ… æ·»åŠ ï¼šä»æ—§æ¨¡å‹åŠ è½½å‚æ•°åˆ°æ–°æ¨¡å‹ï¼ˆå°½å¯èƒ½å¤šåœ°è¿ç§»ï¼‰
def load_partial_policy_weights(old_model_path, new_model):
    if not os.path.exists(old_model_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„ï¼š{old_model_path}")

    print(f"ğŸš€ åŠ è½½æ—§æ¨¡å‹å‚æ•°ä¸­: {old_model_path}")
    old_model = PPO.load(old_model_path, device=new_model.device)
    old_policy = old_model.policy
    new_policy = new_model.policy

    old_state_dict = old_policy.state_dict()
    new_state_dict = new_policy.state_dict()

    loaded_keys = []
    skipped_keys = []

    for name, param in old_state_dict.items():
        if name in new_state_dict and param.shape == new_state_dict[name].shape:
            new_state_dict[name].copy_(param.data)
            loaded_keys.append(name)
        else:
            skipped_keys.append(name)

    print(f"âœ… æˆåŠŸåŠ è½½æƒé‡æ•°: {len(loaded_keys)}")
    if skipped_keys:
        print(f"âš ï¸ è·³è¿‡ä¸åŒ¹é…çš„æƒé‡: {skipped_keys[:5]}{' ...' if len(skipped_keys) > 5 else ''}")

    with torch.no_grad():
        old_logits = old_model.policy.action_net.weight
        new_logits = new_model.policy.action_net.weight
        old_bias = old_model.policy.action_net.bias
        new_bias = new_model.policy.action_net.bias

        # âœ… å¦‚æœ old æ˜¯ 3 ç»´ï¼Œæ–°æ˜¯ 4 ç»´ï¼Œå°±å¤åˆ¶å‰ä¸‰ç»´å‚æ•°
        with torch.no_grad():
            new_logits[:10] = old_logits[:10]
            new_bias[:10] = old_bias[:10]

            new_bias[12] = -5.0

            print(f"âœ… è¿ç§»å‰ {10} logitsï¼ŒåŠ¨ä½œ {12} è®¾ç½® bias={-5}")




if __name__ == "__main__":
    last_save_step = 0

    expert_trajs = load_expert_trajectories(f"./test_result/expert_data/expert_data2.npz")

    args3 = Namespace(
        config_name='1v1/ShootMissile/HierarchyVsBaselineShootBack',
        model_path='./trained_model/shoot_back/ppo_air_combat.zip',
        log_file='./train/result/train_shoot_back2_gail3.log',
        save_path='./test_result/expert_data',
        save_npz=True,
        num_envs=16,
        max_steps=50_001,
    )

    # æ„é€ ç¯å¢ƒ
    env_fns = [make_env(i, args3.config_name, args3.log_file) for i in range(args3.num_envs)]
    venv = SubprocVecEnv(env_fns)
    # æ„å»º reward_netï¼ˆç”¨äº imitation rewardï¼‰
    reward_net = BasicRewardNet(
        observation_space=venv.observation_space,
        action_space=venv.action_space,
        normalize_input_layer=None,
    )

    # åŒ…è£…ç¯å¢ƒä»¥ä½¿ç”¨æ··åˆå¥–åŠ±
    venv_wrapped = MixedRewardEnv(venv, reward_net.predict, imitation_weight=0.9, env_weight=0.1)

    # åˆå§‹åŒ– PPO ç­–ç•¥
    policy_kwargs = dict(
        features_extractor_class=CustomImitationPolicy,
        features_extractor_kwargs={}
    )

    ppo = PPO(
        "MlpPolicy",
        venv_wrapped,
        policy_kwargs=policy_kwargs,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.02,
        verbose=1,
        tensorboard_log="./ppo_air_combat_tb/",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # âœ… åŠ è½½æ—§ç­–ç•¥å‚æ•°ï¼ˆåŒ…æ‹¬å‰ä¸‰ç»´åŠ¨ä½œè¾“å‡ºï¼‰
    load_partial_policy_weights(args3.model_path, ppo)

    # åˆå§‹åŒ– Freezer + TensorBoard Callback
    freezer = Freezer(ppo.policy, num_old_logits=11, unfreeze_step=1_000_000)

    # åˆå§‹åŒ– GAIL trainer
    gail_trainer = GAIL(
        demonstrations=expert_trajs,  # âœ… ä¸éœ€è¦ flatten
        demo_batch_size=64,
        gen_replay_buffer_capacity=2048,
        n_disc_updates_per_round=4,
        venv=venv_wrapped,
        gen_algo=ppo,
        reward_net=reward_net,
        allow_variable_horizon=True,  # âœ… å…è®¸ä¸åŒé•¿åº¦çš„ episode
    )

    # GAIL è®­ç»ƒ
    gail_trainer.train(total_timesteps=4_000_000, callback=gail_custom_callback)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    ppo.save("./trained_model/shoot_back/gail_ppo_policy3.zip")
    print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ° './trained_model/shoot_back/gail_ppo_policy3.zip'")
