import os
import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import Dataset, DataLoader
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from gym import spaces
from gym.spaces import MultiDiscrete
from sklearn.model_selection import train_test_split

from net.net_shoot_imitation import MLPBase, GRULayer
from adapter.adapter_dodge_missile import SB3SingleCombatEnv


# æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ç½‘ç»œï¼ˆç›´æ¥è¾“å‡ºè¿ç»­åŠ¨ä½œï¼‰
class ImitationPolicy(nn.Module):
    def __init__(self, obs_dim):
        super().__init__()
        self.feature_extractor = MLPBase(obs_dim, 128)
        self.gru = GRULayer(128, 128)
        self.action_head = nn.Linear(128, 5)  # è¿ç»­åŠ¨ä½œè¾“å‡º 5ç»´

    def forward(self, x):
        feat = self.feature_extractor(x)
        feat = self.gru(feat)
        raw = self.action_head(feat)
        # å‰3ç»´ tanh [-1, 1]ï¼Œç¬¬4ç»´ sigmoid * 0.5 + 0.4 => [0.4, 0.9]ï¼Œç¬¬5ç»´ sigmoid => [0,1]
        aileron = torch.tanh(raw[:, 0:1])
        elevator = torch.tanh(raw[:, 1:2])
        rudder = torch.tanh(raw[:, 2:3])
        throttle = torch.sigmoid(raw[:, 3:4]) * 0.5 + 0.4
        shoot = torch.sigmoid(raw[:, 4:5])
        return torch.cat([aileron, elevator, rudder, throttle, shoot], dim=-1)

# ä¸“å®¶æ•°æ®é›†
class ExpertDataset(Dataset):
    def __init__(self, data_dir):
        self.obs = []
        self.actions = []
        for file in os.listdir(data_dir):
            if file.endswith(".npz"):
                data = np.load(os.path.join(data_dir, file))
                self.obs.append(data["obs"])
                self.actions.append(data["action"])
        self.obs = np.concatenate(self.obs, axis=0)
        self.actions = np.concatenate(self.actions, axis=0)

    def __len__(self):
        return len(self.obs)

    def __getitem__(self, idx):
        return torch.tensor(self.obs[idx], dtype=torch.float32), \
               torch.tensor(self.actions[idx], dtype=torch.float32)

# è®­ç»ƒå¹¶å¯¼å‡º PPO zip æ¨¡å‹

def train_imitation_and_export(data_dir, env, zip_path="imitation_pretrained.zip", patience=5):
    dataset = ExpertDataset(data_dir)

    # âœ… æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=0.1, shuffle=True)
    train_dataset = torch.utils.data.Subset(dataset, train_idx)
    val_dataset = torch.utils.data.Subset(dataset, val_idx)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64)

    obs_dim = dataset[0][0].shape[0]
    imit_model = ImitationPolicy(obs_dim)
    optim = torch.optim.Adam(imit_model.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()

    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(100):  # æœ€å¤šè®­ç»ƒ 100 è½®
        imit_model.train()
        total_loss = 0
        for obs_batch, act_batch in train_loader:
            pred = imit_model(obs_batch)
            loss = loss_fn(pred, act_batch)
            optim.zero_grad()
            loss.backward()
            optim.step()
            total_loss += loss.item() * obs_batch.size(0)
        train_loss = total_loss / len(train_dataset)

        # éªŒè¯é›† loss
        imit_model.eval()
        with torch.no_grad():
            val_loss = 0
            for obs_batch, act_batch in val_loader:
                pred = imit_model(obs_batch)
                loss = loss_fn(pred, act_batch)
                val_loss += loss.item() * obs_batch.size(0)
            val_loss = val_loss / len(val_dataset)

        print(f"[Epoch {epoch + 1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_sd = imit_model.state_dict()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"â¹ï¸ æ—©åœè§¦å‘ï¼šéªŒè¯é›† {patience} æ¬¡æœªæå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break

    best_model_sd = torch.load('imitation_pretrained_pytorch.pt')

    # æ¢å¤æœ€ä½³å‚æ•°
    imit_model.load_state_dict(best_model_sd)

    # âœ… ä¿å­˜ PyTorch åŸå§‹æ¨¡å‹ï¼ˆé˜²æ­¢ zip å¤±è´¥ï¼‰
    torch.save(best_model_sd, zip_path.replace(".zip", "_pytorch.pt"))
    print(f"ğŸ“¦ å·²ä¿å­˜ä¸ºåŸå§‹ PyTorch æ¨¡å‹: {zip_path.replace('.zip', '_pytorch.pt')}")

    # åˆå§‹åŒ– SB3 PPO æ¨¡å‹
    policy_kwargs = dict(
        features_extractor_class=CustomPolicy
    )
    ppo = PPO("MlpPolicy", env, policy_kwargs=policy_kwargs, verbose=0)

    # æ‹·è´å‚æ•°ï¼ˆåªè¿ç§» feature + gruï¼‰
    imit_sd = imit_model.state_dict()
    ppo.policy.features_extractor.feature_extractor.load_state_dict({
        k.replace("feature_extractor.", ""): v for k, v in imit_sd.items() if "feature_extractor" in k
    }, strict=False)
    ppo.policy.features_extractor.gru_layer.load_state_dict({
        k.replace("gru.", ""): v for k, v in imit_sd.items() if "gru" in k
    }, strict=False)

    # å¼ºåŒ–å­¦ä¹ é˜¶æ®µä¼šé‡æ–°åˆå§‹åŒ–åŠ¨ä½œå¤´ï¼Œæ‰€ä»¥è¿™é‡Œåªè¿ç§»ç‰¹å¾æå–å™¨éƒ¨åˆ†
    ppo.save(zip_path)
    print(f"âœ… å·²ä¿å­˜ä¸º SB3 PPO æ¨¡å‹: {zip_path}")



env = SB3SingleCombatEnv(0, config_name='1v1/DodgeMissile/HierarchyVsBaselineSelf')
train_imitation_and_export(data_dir="render_train/dodge2/imitation", env=env)
