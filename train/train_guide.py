import gym
import gymnasium
import torch
import torch.nn as nn
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CheckpointCallback
from gymnasium import spaces
import argparse
import os
import logging

from LAGmaster.envs.JSBSim.envs import SingleCombatEnvGuide
from net import CustomImitationShootBackPolicy, CustomActorCriticShootBackPolicy, CustomImitationPolicy

# ========== 1. é€‚é… SB3 çš„è‡ªå®šä¹‰ç¯å¢ƒ ==========
class SB3SingleCombatEnv(gymnasium.Env):
    """å°† SingleCombatEnvTest é€‚é…ä¸º SB3 å…¼å®¹çš„ Gym ç¯å¢ƒ"""

    def __init__(self, env_id, config_name):
        super(SB3SingleCombatEnv, self).__init__()
        self.env = SingleCombatEnvGuide(config_name, env_id)  # ä½ çš„åŸå§‹ç¯å¢ƒ
        # obs_shape = self.env.get_obs().shape[0]  # è·å–è§‚æµ‹ç©ºé—´ç»´åº¦
        # act_shape = self.env.get_action_space().shape[0]  # è·å–åŠ¨ä½œç©ºé—´ç»´åº¦
        # ç»§æ‰¿åŸå§‹ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´å’Œè§‚å¯Ÿç©ºé—´
        # self.action_space = self.env.action_space

        # æå–åŸå§‹ç¯å¢ƒçš„ action_space
        if isinstance(self.env.action_space, spaces.Tuple):
            # è·å–æ‰€æœ‰ç¦»æ•£åŠ¨ä½œç©ºé—´çš„ç»´åº¦
            action_dims = []
            for space in self.env.action_space.spaces:
                if isinstance(space, spaces.MultiDiscrete):
                    action_dims.extend(space.nvec)  # å±•å¼€ MultiDiscrete
                elif isinstance(space, spaces.Discrete):
                    action_dims.append(space.n)  # Discrete ç›´æ¥æ·»åŠ 
                else:
                    raise ValueError("Unsupported action space type: {}".format(type(space)))

            # è½¬æ¢ä¸º MultiDiscrete
            self.action_space = spaces.MultiDiscrete(action_dims)
        else:
            # raise ValueError("Unexpected action space type: {}".format(type(self.env.action_space)))
            self.action_space = self.env.action_space

        self.observation_space = self.env.observation_space

        # # å®šä¹‰ Gym å…¼å®¹çš„è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´
        # self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_shape,), dtype=np.float32)
        # self.action_space = spaces.Box(low=-1, high=1, shape=(act_shape,), dtype=np.float32)

    def step(self, action):
        # å°†é•¿åº¦ä¸º 4 çš„åŠ¨ä½œè½¬æ¢ä¸ºé•¿åº¦ä¸º (1,4) çš„åŠ¨ä½œ
        # actual_action = action.reshape(-1, 3)  # å–ç¬¬ä¸€ä¸ªå€¼
        # å› ä¸ºå†…éƒ¨çš„ç¯å¢ƒï¼Œå‡è®¾å¯èƒ½æœ‰å¤šæ¶é£æœºåœ¨æ§åˆ¶ï¼Œæ‰€æœ‰ç¬¬ä¸€ä½éƒ½åŠ äº†ä¸ªåºå·
        # reward å’Œdonesä»€ä¹ˆçš„ç›´æ¥å–å€¼

        action = np.expand_dims(action, axis=0) if action.ndim == 1 else action

        obs, rewards, dones, info = self.env.step(action)

        timeout = info.get('timeout', False)

        observation, reward, terminated, truncated, info = obs[0], rewards.item(), dones.item(), timeout, info

        # logging.info('test')

        return observation, reward, terminated, truncated, info

    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒï¼Œæ”¯æŒ `seed` ä»¥é€‚é… SB3"""
        super().reset(seed=seed)  # è®© Gym å…¼å®¹ SB3 çš„ `seed`
        obs = self.env.reset()
        observation = obs[0]
        return observation, None

    def close(self):
        return self.env.close()

    def render(self, mode="txt", filepath='./JSBSimRecording.txt.acmi', tacview=None):
        self.env.render(mode=mode, filepath=filepath, tacview=tacview)


def get_config():
    parser = argparse.ArgumentParser(description="PPO Training for Single Combat Dodge Missile Scenario")

    # ç¯å¢ƒç›¸å…³å‚æ•°
    parser.add_argument("--env-name", type=str, default="SingleCombat", help="ç¯å¢ƒåç§°")
    parser.add_argument("--algorithm-name", type=str, default="ppo", help="ç®—æ³•åç§°")
    parser.add_argument("--scenario-name", type=str, default="1v1/DodgeMissile/HierarchyVsBaseline", help="åœºæ™¯åç§°")
    parser.add_argument("--experiment-name", type=str, default="1v1", help="å®éªŒåç§°")

    # è®­ç»ƒè®¾ç½®
    parser.add_argument("--seed", type=int, default=1, help="éšæœºç§å­")
    parser.add_argument("--n-training-threads", type=int, default=1, help="è®­ç»ƒæ—¶çš„çº¿ç¨‹æ•°")
    parser.add_argument("--n-rollout-threads", type=int, default=1, help="é‡‡æ ·çº¿ç¨‹æ•°")
    parser.add_argument("--cuda", action="store_true", help="æ˜¯å¦ä½¿ç”¨ CUDA åŠ é€Ÿ")

    # è®°å½•ä¸ä¿å­˜
    parser.add_argument("--log-interval", type=int, default=1, help="æ—¥å¿—è®°å½•é—´éš”ï¼ˆå•ä½ï¼šå›åˆï¼‰")
    parser.add_argument("--save-interval", type=int, default=1, help="æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆå•ä½ï¼šå›åˆï¼‰")

    # è¯„ä¼°è®¾ç½®
    parser.add_argument("--n-choose-opponents", type=int, default=1, help="é€‰æ‹©çš„å¯¹æ‰‹æ•°é‡")
    parser.add_argument("--use-eval", action="store_true", help="æ˜¯å¦ä½¿ç”¨è¯„ä¼°æ¨¡å¼")
    parser.add_argument("--n-eval-rollout-threads", type=int, default=1, help="è¯„ä¼°æ—¶çš„ rollout çº¿ç¨‹æ•°")
    parser.add_argument("--eval-interval", type=int, default=1, help="è¯„ä¼°é—´éš”")
    parser.add_argument("--eval-episodes", type=int, default=1, help="æ¯æ¬¡è¯„ä¼°çš„ episode æ•°")

    # PPO è®­ç»ƒè¶…å‚æ•°
    parser.add_argument("--num-mini-batch", type=int, default=5, help="PPO çš„ mini-batch æ•°é‡")
    parser.add_argument("--buffer-size", type=int, default=200, help="ç»éªŒç¼“å†²åŒºå¤§å°")
    parser.add_argument("--num-env-steps", type=float, default=1e8, help="è®­ç»ƒç¯å¢ƒæ­¥æ•°")
    parser.add_argument("--lr", type=float, default=3e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--gamma", type=float, default=0.99, help="æŠ˜æ‰£å› å­")
    parser.add_argument("--ppo-epoch", type=int, default=4, help="PPO è®­ç»ƒçš„ epoch æ•°")
    parser.add_argument("--clip-params", type=float, default=0.2, help="PPO è£å‰ªå‚æ•°")
    parser.add_argument("--max-grad-norm", type=float, default=2, help="æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°")
    parser.add_argument("--entropy-coef", type=float, default=1e-3, help="ç†µæ­£åˆ™ç³»æ•°")

    # ç¥ç»ç½‘ç»œç»“æ„
    parser.add_argument("--hidden-size", type=int, nargs="+", default=[128, 128], help="Actor-Critic ç½‘ç»œçš„éšè—å±‚å¤§å°")
    parser.add_argument("--act-hidden-size", type=int, nargs="+", default=[128, 128], help="Actor ç½‘ç»œçš„éšè—å±‚å¤§å°")
    parser.add_argument("--recurrent-hidden-size", type=int, default=128, help="RNN éšè—å±‚å¤§å°")
    parser.add_argument("--recurrent-hidden-layers", type=int, default=1, help="RNN éšè—å±‚æ•°")
    parser.add_argument("--data-chunk-length", type=int, default=8, help="RNN è®­ç»ƒæ—¶çš„æ•°æ®å—é•¿åº¦")

    return parser


class EnvIDFilter(logging.Filter):
    def __init__(self, env_id):
        super().__init__()
        self.env_id = env_id

    def filter(self, record):
        record.env_id = f"{self.env_id}"
        return True


def setup_logging(env_id=0, log_file=None):
    """é…ç½® loggingï¼Œè®©æ—¥å¿—æ—¢è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œåˆå†™å…¥æ–‡ä»¶ï¼Œæ ‡æ˜ ENV ID"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    # åˆ›å»º Filterï¼Œç”¨äºæ³¨å…¥ env_id
    env_filter = EnvIDFilter(env_id)

    # æ—¥å¿—æ ¼å¼å¸¦ env_id
    formatter = logging.Formatter("%(asctime)s - %(levelname)s [ENV %(env_id)s] - %(message)s")

    # ç»ˆç«¯ handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    console_handler.addFilter(env_filter)

    # æ–‡ä»¶ handler
    file_handler = logging.FileHandler(log_file, mode="a")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    file_handler.addFilter(env_filter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    logging.info(f"Logger for ENV {env_id} initialized, log path: {log_file}")


# ========== 6. è®­ç»ƒ PPO ==========
# =================== è®­ç»ƒä¸»ç¨‹åº ===================
if __name__ == "__main__":
    # å‚æ•°
    num_envs = 16     
    log_file = "./train/result/train_guide.log"
    model_path = "" # "./trained_model/shoot_imitation/ppo_air_combat_imi.zip"
    pretrained_pt_path = ""  # "./trained_model/imitation_shoot/imitation_pretrained_pytorch.pt"

    # å¤šè¿›ç¨‹ç¯å¢ƒåˆ›å»º
    def make_env(env_id):
        setup_logging(env_id, log_file)
        return SB3SingleCombatEnv(env_id, config_name='1v1/ShootMissile/HierarchyVsBaselineGuide')

    env = SubprocVecEnv([lambda env_id=i: make_env(env_id) for i in range(num_envs)])

    if os.path.exists(model_path):
        policy_kwargs = dict(
            features_extractor_class=CustomImitationPolicy,
            features_extractor_kwargs={}
        )
        print("âœ… åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ...")
        model = PPO.load(
            model_path,
            env=env,
            policy_kwargs=policy_kwargs,
            tensorboard_log="./ppo_air_combat_tb/",
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
    else:
        policy_kwargs = dict(
            features_extractor_class=CustomImitationPolicy,
            features_extractor_kwargs={}
        )

        model = PPO(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.02,
            verbose=1,
            tensorboard_log="./ppo_air_combat_tb/",
            device="cuda" if torch.cuda.is_available() else "cpu"
        )

        # ğŸ”„ åŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼ˆmlp, gru, action_headï¼‰
        if os.path.exists(pretrained_pt_path):
            print("ğŸ”„ åŠ è½½æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒå‚æ•°...")
            policy = model.policy

            pretrained = torch.load(pretrained_pt_path, map_location="cpu")

            # MLP
            mlp_state = {k.replace("feature_extractor.", ""): v for k, v in pretrained.items() if
                         k.startswith("feature_extractor.")}
            policy.features_extractor.mlp.load_state_dict(mlp_state)

            # GRUï¼ˆæ³¨æ„åŠ è½½çš„æ˜¯ GRULayer.gruï¼‰
            gru_state = {k.replace("gru.", ""): v for k, v in pretrained.items() if k.startswith("gru.")}
            policy.features_extractor.gru.gru.load_state_dict(gru_state)

            # action_head
            act_state = {k.replace("action_head.", ""): v for k, v in pretrained.items() if
                         k.startswith("action_head.")}
            policy.action_net.load_state_dict(act_state)

            print("âœ… æ¨¡ä»¿å­¦ä¹ å‚æ•°åŠ è½½æˆåŠŸï¼")

    # åˆ›å»º checkpoint å›è°ƒï¼Œæ¯ 10 ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡
    checkpoint_callback = CheckpointCallback(
        save_freq=10_000,  # æ¯ 1*num_env ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡
        save_path="./trained_model/guide_checkpoints/",  # ä¿å­˜æ–‡ä»¶å¤¹
        name_prefix="ppo_air_combat_guide"  # æ–‡ä»¶åå‰ç¼€
    )

    # å¼€å§‹è®­ç»ƒï¼ŒåŒæ—¶è®°å½• TensorBoard å’Œä¿å­˜ä¸­é—´æ¨¡å‹
    model.learn(
        total_timesteps=3_000_000,
        tb_log_name="test_guide",
        callback=checkpoint_callback
    )

    # æœ€ç»ˆè®­ç»ƒå®Œæˆåä¿å­˜ä¸€æ¬¡å®Œæ•´æ¨¡å‹
    model.save("./trained_model/guide/ppo_air_combat")

    # å…³é—­ç¯å¢ƒ
    env.close()
