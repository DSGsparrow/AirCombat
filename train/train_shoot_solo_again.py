import os
import argparse
import numpy as np
import torch
import gymnasium as gym  # æ–°ç‰ˆ gymnasium
from gymnasium import spaces

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
import torch.nn as nn

from env_factory.env_factory_selfplay import make_env, make_normal_env

# === ä½ å·²æœ‰çš„ï¼šparse_args / make_normal_env ç­‰ ===
# from your_code import parse_args, make_normal_env

MANEUVER_MODEL_PATH = "trained_model/shoot_static3/final_model.zip"


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="1v1/ShootMissile/HierarchySelfPlayShoot")
    parser.add_argument("--target_state", type=int, default=0)

    # åŸºæœ¬è·¯å¾„
    parser.add_argument("--log_file", type=str, default="./train/result/train_shoot_static_solo_gap.log")
    parser.add_argument("--model_path", type=str, default="")
    parser.add_argument("--pretrained_pt_path", type=str, default="")
    parser.add_argument("--checkpoint_path", type=str, default="./trained_model/shoot_static_solo_gap/checkpoints/")
    parser.add_argument("--tb_log", type=str, default="./ppo_air_combat_sp_tb/")
    parser.add_argument("--save_model_path", type=str, default="./trained_model/shoot_static_solo_gap")
    parser.add_argument("--model_dir", type=str, default="./model_pool/shoot_static")

    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--fly_model_path", type=str, default="trained_model/shoot_back_t2/ppo_air_combat.zip")
    parser.add_argument("--fire_model_path", type=str, default="./trained_model/shoot_solo5/ppo_air_combat.zip")
    parser.add_argument("--guide_model_path", type=str, default="trained_model/guide/ppo_air_combat.zip")
    parser.add_argument("--dodge_model_path", type=str, default="trained_model/dodge_missile/ppo_air_combat_dodge4.zip")

    # ç¯å¢ƒå‚æ•°
    parser.add_argument("--history_len", type=int, default=10)
    parser.add_argument("--raw_obs_dim", type=int, default=21)
    parser.add_argument("--fly_act_dim", type=int, default=3)
    parser.add_argument("--fire_act_dim", type=int, default=2)
    parser.add_argument("--warmup_action", nargs='+', type=float, default=[1, 2, 1, 0.0, 0.0])

    # å¤šçº¿ç¨‹
    parser.add_argument("--num_envs", type=int, default=16)

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--total_timesteps", type=int, default=8_000_000)
    parser.add_argument("--save_interval", type=int, default=20_000)
    parser.add_argument("--learning_rate", type=float, default=1e-4)  # 3e-4
    parser.add_argument("--n_steps", type=int, default=2048)
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--n_epochs", type=int, default=10)
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--gae_lambda", type=float, default=0.95)
    parser.add_argument("--clip_range", type=float, default=0.15)
    parser.add_argument("--ent_coef", type=float, default=0.02)

    # Transformer ç½‘ç»œå‚æ•°
    parser.add_argument("--embed_dim", type=int, default=64)
    parser.add_argument("--num_heads", type=int, default=4)
    parser.add_argument("--num_layers", type=int, default=3)
    parser.add_argument("--dropout", type=float, default=0.1)

    return parser.parse_args()



class HybridActionWrapper(gym.Wrapper):
    """
    åªè®­ç»ƒâ€œå‘å°„å¸ƒå°”å€¼â€ï¼Œå‰ä¸‰ç»´æœºåŠ¨åŠ¨ä½œæ’ä¸º 0ã€‚
    æ–°å¢åŠŸèƒ½ï¼šå†³ç­–é—´éš”ï¼ˆaction repeat / frame skipï¼‰
      - å¤–éƒ¨æ™ºèƒ½ä½“æ¯æ¬¡å†³ç­–åï¼Œç¯å¢ƒå†…éƒ¨è‡ªåŠ¨æ‰§è¡Œ decision_interval æ­¥
      - æœ¬ wrapper ä¼šç´¯è®¡è¿™å‡ æ­¥çš„ reward å¹¶è¿”å›æœ€åä¸€æ­¥çš„ obs / done / info

    å‚æ•°
    ----
    decision_interval : int >= 1
        é—´éš”æ­¥æ•°ï¼ˆä¾‹å¦‚ 5 è¡¨ç¤ºæ¯æ¬¡å†³ç­–åè‡ªåŠ¨èµ° 5 æ­¥ï¼‰
    edge_fire : bool
        True  ï¼šä»…åœ¨é—´éš”å†…çš„ç¬¬ä¸€æ­¥å°† fire=1 ä¼ å…¥åº•å±‚ï¼Œå…¶ä½™æ­¥å¼ºåˆ¶ fire=0ï¼ˆé¿å…å¤šæ¬¡å‘å°„ï¼‰
        False ï¼šåœ¨æ•´ä¸ªé—´éš”å†…é‡å¤ç›¸åŒçš„ fire å€¼ï¼ˆå¯èƒ½å¯¼è‡´å¤šæ¬¡å‘å°„ï¼‰
    """

    def __init__(self, env, decision_interval: int = 1, edge_fire: bool = True):
        super().__init__(env)
        assert isinstance(decision_interval, int) and decision_interval >= 1, \
            f"decision_interval å¿…é¡»æ˜¯ >=1 çš„æ•´æ•°ï¼Œç»™å®š {decision_interval}"
        self.decision_interval = decision_interval
        self.edge_fire = bool(edge_fire)

        # å¯¹å¤–ï¼šè§‚æµ‹ä¸å˜ï¼Œåªè®­ç»ƒäºŒå…ƒå‘å°„
        self.observation_space = env.observation_space
        self.action_space = spaces.Discrete(2)  # 0=ä¸å‘å°„, 1=å‘å°„

        # åº•å±‚åŠ¨ä½œç©ºé—´ä¿¡æ¯ï¼ˆâ‰¥4ç»´ï¼šå‰ä¸‰ç»´æœºåŠ¨ï¼Œæœ€åä¸€ç»´ fireï¼‰
        self._is_multidiscrete = isinstance(env.action_space, spaces.MultiDiscrete)
        self._is_box = isinstance(env.action_space, spaces.Box)

        if self._is_multidiscrete:
            nvec = env.action_space.nvec
            assert len(nvec) >= 4, f"æœŸæœ›åº•å±‚åŠ¨ä½œâ‰¥4ç»´ï¼Œç°åœ¨æ˜¯ {len(nvec)}"
        elif self._is_box:
            assert env.action_space.shape[0] >= 4, \
                f"æœŸæœ›åº•å±‚è¿ç»­åŠ¨ä½œâ‰¥4ç»´ï¼Œç°åœ¨æ˜¯ {env.action_space.shape}"
        else:
            raise TypeError("åº•å±‚åŠ¨ä½œç©ºé—´æ—¢ä¸æ˜¯ MultiDiscrete ä¹Ÿä¸æ˜¯ Boxã€‚")

        self._last_obs = None

    # è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢
    def set_decision_interval(self, n: int):
        assert isinstance(n, int) and n >= 1, f"decision_interval å¿…é¡» >=1ï¼Œç»™å®š {n}"
        self.decision_interval = n

    def set_edge_fire(self, flag: bool):
        self.edge_fire = bool(flag)

    # ---------- å…¼å®¹ gym / gymnasium ----------
    @staticmethod
    def _is_gymnasium_step_tuple(t):
        # gymnasium: (obs, reward, terminated, truncated, info)
        return isinstance(t, tuple) and len(t) == 5

    # ---------- æ„é€ åº•å±‚åŠ¨ä½œï¼ˆå‰ä¸‰ç»´ 0ï¼Œæœ€åä¸€ç»´ä¸º fireï¼‰ ----------
    def _build_action(self, fire_bool: int):
        fire = int(fire_bool)

        if self._is_multidiscrete:
            out = np.zeros_like(self.env.action_space.nvec, dtype=np.int64)
            out[:3] = 0
            out[3] = fire
            # åˆæ³•èŒƒå›´
            out = np.minimum(out, self.env.action_space.nvec - 1)
            out = np.maximum(out, 0)
            return out

        elif self._is_box:
            out = np.zeros((self.env.action_space.shape[0],), dtype=np.float32)
            out[:3] = 0.0
            out[3] = float(fire)
            return np.clip(out, self.env.action_space.low, self.env.action_space.high)

        else:
            raise RuntimeError("æœªçŸ¥çš„åŠ¨ä½œç©ºé—´ç±»å‹ã€‚")

    def reset(self, **kwargs):
        ret = self.env.reset(**kwargs)
        if isinstance(ret, tuple) and len(ret) == 2:  # gymnasium
            obs, info = ret
            self._last_obs = obs
            return obs, info
        else:  # gym
            self._last_obs = ret
            return ret

    def step(self, action):
        # è§£æå¤–éƒ¨äºŒå…ƒåŠ¨ä½œ
        if isinstance(action, (np.ndarray, list, tuple)):
            fire_bool = int(np.asarray(action).reshape(-1)[0])
        else:
            fire_bool = int(action)

        total_reward = 0.0
        any_launch = False
        last_info = {}
        terminated = False
        truncated = False
        done = False
        obs = self._last_obs

        # åœ¨é—´éš”å†…æ‰§è¡Œå¤šæ­¥
        for k in range(self.decision_interval):
            # edge_fire=True æ—¶ï¼Œä»…ç¬¬ä¸€æ­¥ä¼  fire_boolï¼Œå…¶ä½™æ­¥å¼ºåˆ¶ 0
            this_fire = fire_bool if (k == 0 or not self.edge_fire) else 0

            full_action = self._build_action(this_fire)
            step_out = self.env.step(full_action)

            if self._is_gymnasium_step_tuple(step_out):
                obs, r, terminated, truncated, info = step_out
                done = terminated or truncated
            else:
                obs, r, done, info = step_out
                # ä¸ºç»Ÿä¸€ï¼Œä¸‹ä¸¤è¡Œæä¾›å ä½
                terminated, truncated = done, False

            self._last_obs = obs
            total_reward += float(r)
            last_info = info if isinstance(info, dict) else {}
            any_launch = any_launch or bool(last_info.get("launch", False))

            if done:  # è‹¥æå‰ç»“æŸï¼Œè·³å‡º
                break

        # è¡¥å……ä¸€äº›ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        if isinstance(last_info, dict):
            last_info = dict(last_info)
            last_info["decision_interval"] = self.decision_interval
            last_info["interval_steps_executed"] = k + 1  # å®é™…æ‰§è¡Œäº†å‡ æ­¥
            last_info["interval_reward_sum"] = total_reward
            last_info["interval_any_launch"] = any_launch
        else:
            last_info = {
                "decision_interval": self.decision_interval,
                "interval_steps_executed": k + 1,
                "interval_reward_sum": total_reward,
                "interval_any_launch": any_launch,
            }

        # è¿”å›æœ€åä¸€æ­¥çš„ obs / flagsï¼Œä»¥åŠç´¯è®¡å¥–åŠ±
        if self._is_gymnasium_step_tuple(step_out):
            return obs, total_reward, terminated, truncated, last_info
        else:
            return obs, total_reward, done, last_info
# class HybridActionWrapper(gym.Wrapper):
#     """
#     åªè®­ç»ƒâ€œå‘å°„å¸ƒå°”å€¼â€ï¼Œå‰ä¸‰ç»´æœºåŠ¨åŠ¨ä½œæ’ä¸º 0ã€‚
#     è‹¥ action=1 ä¸”åº•å±‚è¿”å› info['launch']=Trueï¼š
#       - rollout_after_launch=True: åœ¨å†…éƒ¨ roll out åˆ°å›åˆç»“æŸï¼Œç´¯è®¡å¥–åŠ±ä¸€æ¬¡æ€§è¿”å›
#       - rollout_after_launch=False: ä¸è·³è¿‡ï¼ŒæŒ‰æ™®é€šä¸€æ­¥è¿”å›
#     """
#
#     def __init__(self, env, rollout_after_launch: bool = True):
#         super().__init__(env)
#         self.rollout_after_launch = rollout_after_launch
#
#         # å¯¹å¤–ï¼šè§‚æµ‹ä¸å˜ï¼Œåªè®­ç»ƒäºŒå…ƒå‘å°„
#         self.observation_space = env.observation_space
#         self.action_space = spaces.Discrete(2)  # 0=ä¸å‘å°„, 1=å‘å°„
#
#         # åº•å±‚åŠ¨ä½œç©ºé—´ä¿¡æ¯
#         self._is_multidiscrete = isinstance(env.action_space, spaces.MultiDiscrete)
#         self._is_box = isinstance(env.action_space, spaces.Box)
#
#         if self._is_multidiscrete:
#             nvec = env.action_space.nvec
#             assert len(nvec) >= 4, f"æœŸæœ›åº•å±‚åŠ¨ä½œâ‰¥4ç»´ï¼Œç°åœ¨æ˜¯ {len(nvec)}"
#         elif self._is_box:
#             assert env.action_space.shape[0] >= 4, \
#                 f"æœŸæœ›åº•å±‚è¿ç»­åŠ¨ä½œâ‰¥4ç»´ï¼Œç°åœ¨æ˜¯ {env.action_space.shape}"
#         else:
#             raise TypeError("åº•å±‚åŠ¨ä½œç©ºé—´æ—¢ä¸æ˜¯ MultiDiscrete ä¹Ÿä¸æ˜¯ Boxã€‚")
#
#         self._last_obs = None
#
#     # è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢
#     def set_rollout_after_launch(self, flag: bool):
#         self.rollout_after_launch = bool(flag)
#
#     # ---------- æ„é€ åº•å±‚åŠ¨ä½œï¼ˆå‰ä¸‰ç»´ 0ï¼Œæœ€åä¸€ç»´ä¸º fireï¼‰ ----------
#     def _build_action(self, fire_bool: int):
#         fire = int(fire_bool)
#
#         if self._is_multidiscrete:
#             out = np.zeros_like(self.env.action_space.nvec, dtype=np.int64)
#             out[:3] = 0
#             out[3] = fire
#             # åˆæ³•èŒƒå›´
#             out = np.minimum(out, self.env.action_space.nvec - 1)
#             out = np.maximum(out, 0)
#             return out
#
#         elif self._is_box:
#             out = np.zeros((self.env.action_space.shape[0],), dtype=np.float32)
#             out[:3] = 0.0
#             out[3] = float(fire)
#             return np.clip(out, self.env.action_space.low, self.env.action_space.high)
#
#         else:
#             raise RuntimeError("æœªçŸ¥çš„åŠ¨ä½œç©ºé—´ç±»å‹ã€‚")
#
#     # ---------- å…¼å®¹ gym / gymnasium ----------
#     @staticmethod
#     def _is_gymnasium_step_tuple(t):
#         # gymnasium: (obs, reward, terminated, truncated, info)
#         return isinstance(t, tuple) and len(t) == 5
#
#     def reset(self, **kwargs):
#         ret = self.env.reset(**kwargs)
#         if isinstance(ret, tuple) and len(ret) == 2:
#             obs, info = ret
#             self._last_obs = obs
#             return obs, info
#         else:
#             self._last_obs = ret
#             return ret
#
#     def step(self, action):
#         # ä¸Šå±‚åªä¼  0/1
#         if isinstance(action, (np.ndarray, list, tuple)):
#             fire_bool = int(np.asarray(action).reshape(-1)[0])
#         else:
#             fire_bool = int(action)
#
#         # ç¬¬ä¸€æ­¥ï¼šæŒ‰ä¸Šå±‚é€‰æ‹©æ˜¯å¦å‘å°„
#         full_action = self._build_action(fire_bool)
#         step_out = self.env.step(full_action)
#
#         if self._is_gymnasium_step_tuple(step_out):
#             obs, reward, terminated, truncated, info = step_out
#             self._last_obs = obs
#             launched = bool(info.get("launch", False))
#             done = terminated or truncated
#         else:
#             obs, reward, done, info = step_out
#             self._last_obs = obs
#             launched = bool(info.get("launch", False))
#             terminated, truncated = done, False  # ç»Ÿä¸€ç”¨
#
#         # æœªå‘å°„ï¼šç›´æ¥è¿”å›æœ¬æ­¥
#         if not launched:
#             return (obs, reward, terminated, truncated, info) if self._is_gymnasium_step_tuple(step_out) \
#                    else (obs, reward, done, info)
#
#         # å‘å°„äº†ï¼Œä½†é€‰æ‹©â€œä¸è·³è¿‡â€ï¼šè¿”å›æœ¬æ­¥ï¼ˆä¿æŒä¸åº•å±‚ä¸€è‡´ï¼‰
#         if not self.rollout_after_launch:
#             return (obs, reward, terminated, truncated, info) if self._is_gymnasium_step_tuple(step_out) \
#                    else (obs, reward, done, info)
#
#         # å‘å°„ä¸”â€œè·³è¿‡åç»­â€ï¼šå†…éƒ¨ roll out åˆ°å›åˆç»“æŸå¹¶ç´¯åŠ å¥–åŠ±
#         cum_reward = float(reward)
#         rolled_steps = 0
#
#         while True:
#             if self._is_gymnasium_step_tuple(step_out):
#                 if terminated or truncated:
#                     break
#             else:
#                 if done:
#                     break
#
#             # åç»­å›ºå®š fire=0ï¼Œå‰ä¸‰ç»´ä»ä¸º 0
#             full_action = self._build_action(0)
#             step_out = self.env.step(full_action)
#
#             if self._is_gymnasium_step_tuple(step_out):
#                 obs, r, terminated, truncated, info = step_out
#                 self._last_obs = obs
#                 cum_reward += float(r)
#                 rolled_steps += 1
#             else:
#                 obs, r, done, info = step_out
#                 self._last_obs = obs
#                 cum_reward += float(r)
#                 rolled_steps += 1
#                 terminated, truncated = done, False  # ä»…ä¸ºç»Ÿä¸€æ ‡è®°
#
#         # åœ¨æœ€åä¸€æ­¥çš„ info é‡Œè¡¥å……ä¸€äº›è®°å½•ï¼ˆåªåœ¨â€œè·³è¿‡â€æ¨¡å¼ä¸‹æ·»åŠ ï¼‰
#         info = dict(info) if isinstance(info, dict) else {}
#         info.setdefault("launched", True)
#         info["rolled_out_steps"] = rolled_steps
#         info["cum_reward_after_launch"] = cum_reward
#
#         # è¿”å›â€œç»ˆæ­¢æ—¶åˆ»â€çš„ obs / flagsï¼Œä»¥åŠç´¯è®¡å¥–åŠ±
#         if self._is_gymnasium_step_tuple(step_out):
#             return obs, cum_reward, terminated, truncated, info
#         else:
#             return obs, cum_reward, True, info




def make_wrapped_env(env_id, args):
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå•ä¸ªåº•å±‚ env å¹¶åŒ…ä¸Š HybridActionWrapperã€‚"""
    base = make_normal_env(env_id, args)  # ä½ åŸæ¥çš„ç¯å¢ƒåˆ›å»º
    wrapped = HybridActionWrapper(base, decision_interval=5, edge_fire=True)
    return wrapped


def main_shoot_static():
    args = parse_args()
    os.makedirs(args.model_dir, exist_ok=True)

    # === ä¿®æ”¹å¤„ï¼šç”¨åŒ…è£…åçš„å·¥å‚å‡½æ•°ï¼Œä»ç„¶æ”¯æŒå¤šè¿›ç¨‹ SubprocVecEnv ===
    env_fns = [lambda env_id=i: make_wrapped_env(env_id, args) for i in range(args.num_envs)]
    env = SubprocVecEnv(env_fns)

    # Checkpoint å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=args.save_interval,
        save_path=args.checkpoint_path,
        name_prefix="ppo_model"
    )

    # âœ… åªè®­ç»ƒâ€œå‘å°„å¸ƒå°”å€¼â€çš„ PPOï¼Œå› æ­¤ policy ä»ç”¨ MLPï¼Œä½†åŠ¨ä½œç©ºé—´æ¥è‡ª wrapper çš„ Discrete(2)
    policy_kwargs = dict(
        net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])],
        activation_fn=nn.ReLU
    )

    # åŠ è½½æˆ–æ–°å»ºæ¨¡å‹ï¼ˆè¿™é‡ŒæŒ‡â€œå‘å°„å†³ç­–â€æ¨¡å‹ï¼‰
    if os.path.exists(args.model_path):
        print("âœ… åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ...")
        model = PPO.load(
            args.model_path,
            env=env,
            tensorboard_log=args.tb_log,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
    else:
        print("ğŸ†• æ²¡æœ‰æ—§æ¨¡å‹ï¼Œåˆ›å»ºæ–° PPO æ¨¡å‹ï¼ˆä»…è®­ç»ƒå‘å°„å¸ƒå°”å€¼ï¼‰")
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=args.learning_rate,
            n_steps=args.n_steps,
            batch_size=args.batch_size,
            n_epochs=args.n_epochs,
            gamma=args.gamma,
            gae_lambda=args.gae_lambda,
            clip_range=args.clip_range,
            ent_coef=args.ent_coef,
            verbose=1,
            tensorboard_log=args.tb_log,
            device="cuda" if torch.cuda.is_available() else "cpu",
            policy_kwargs=policy_kwargs
        )

    # è®­ç»ƒï¼ˆå…¶ä½™ä¸è¿‡å»ä¸€è‡´ï¼‰
    model.learn(
        total_timesteps=args.total_timesteps,
        callback=checkpoint_callback,
        tb_log_name="ppo_run"
    )

    # æœ€åä¿å­˜ä¸€æ¬¡
    final_model_path = os.path.join(args.save_model_path, "final_model.zip")
    model.save(final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ° {final_model_path}")


if __name__ == "__main__":
    main_shoot_static()