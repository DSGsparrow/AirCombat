import gym
import gymnasium
import torch
import torch.nn as nn
import numpy as np

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CheckpointCallback

from gymnasium import spaces
import argparse
import os
import logging

from net.net_shoot_missile import MLPBase, GRULayer, ACTLayer, CustomPolicy
from adapter.adapter_dodge_missile import SB3SingleCombatEnv

from LAGmaster.envs.JSBSim.envs import SingleCombatEnv, SingleControlEnv, SingleCombatEnvTest


def get_config():
    parser = argparse.ArgumentParser(description="PPO Training for Single Combat Dodge Missile Scenario")

    # ç¯å¢ƒç›¸å…³å‚æ•°
    parser.add_argument("--env-name", type=str, default="SingleCombat", help="ç¯å¢ƒåç§°")
    parser.add_argument("--algorithm-name", type=str, default="ppo", help="ç®—æ³•åç§°")
    parser.add_argument("--scenario-name", type=str, default="1v1/DodgeMissile/HierarchyVsBaseline", help="åœºæ™¯åç§°")
    parser.add_argument("--experiment-name", type=str, default="1v1", help="å®éªŒåç§°")

    # è®­ç»ƒè®¾ç½®
    parser.add_argument("--seed", type=int, default=1, help="éšæœºç§å­")
    parser.add_argument("--n-training-threads", type=int, default=1, help="è®­ç»ƒæ—¶çš„çº¿ç¨‹æ•°")
    parser.add_argument("--n-rollout-threads", type=int, default=1, help="é‡‡æ ·çº¿ç¨‹æ•°")
    parser.add_argument("--cuda", action="store_true", help="æ˜¯å¦ä½¿ç”¨ CUDA åŠ é€Ÿ")

    # è®°å½•ä¸ä¿å­˜
    parser.add_argument("--log-interval", type=int, default=1, help="æ—¥å¿—è®°å½•é—´éš”ï¼ˆå•ä½ï¼šå›åˆï¼‰")
    parser.add_argument("--save-interval", type=int, default=1, help="æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆå•ä½ï¼šå›åˆï¼‰")

    # è¯„ä¼°è®¾ç½®
    parser.add_argument("--n-choose-opponents", type=int, default=1, help="é€‰æ‹©çš„å¯¹æ‰‹æ•°é‡")
    parser.add_argument("--use-eval", action="store_true", help="æ˜¯å¦ä½¿ç”¨è¯„ä¼°æ¨¡å¼")
    parser.add_argument("--n-eval-rollout-threads", type=int, default=1, help="è¯„ä¼°æ—¶çš„ rollout çº¿ç¨‹æ•°")
    parser.add_argument("--eval-interval", type=int, default=1, help="è¯„ä¼°é—´éš”")
    parser.add_argument("--eval-episodes", type=int, default=1, help="æ¯æ¬¡è¯„ä¼°çš„ episode æ•°")

    # PPO è®­ç»ƒè¶…å‚æ•°
    parser.add_argument("--num-mini-batch", type=int, default=5, help="PPO çš„ mini-batch æ•°é‡")
    parser.add_argument("--buffer-size", type=int, default=200, help="ç»éªŒç¼“å†²åŒºå¤§å°")
    parser.add_argument("--num-env-steps", type=float, default=1e8, help="è®­ç»ƒç¯å¢ƒæ­¥æ•°")
    parser.add_argument("--lr", type=float, default=3e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--gamma", type=float, default=0.99, help="æŠ˜æ‰£å› å­")
    parser.add_argument("--ppo-epoch", type=int, default=4, help="PPO è®­ç»ƒçš„ epoch æ•°")
    parser.add_argument("--clip-params", type=float, default=0.2, help="PPO è£å‰ªå‚æ•°")
    parser.add_argument("--max-grad-norm", type=float, default=2, help="æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°")
    parser.add_argument("--entropy-coef", type=float, default=1e-3, help="ç†µæ­£åˆ™ç³»æ•°")

    # ç¥ç»ç½‘ç»œç»“æ„
    parser.add_argument("--hidden-size", type=int, nargs="+", default=[128, 128], help="Actor-Critic ç½‘ç»œçš„éšè—å±‚å¤§å°")
    parser.add_argument("--act-hidden-size", type=int, nargs="+", default=[128, 128], help="Actor ç½‘ç»œçš„éšè—å±‚å¤§å°")
    parser.add_argument("--recurrent-hidden-size", type=int, default=128, help="RNN éšè—å±‚å¤§å°")
    parser.add_argument("--recurrent-hidden-layers", type=int, default=1, help="RNN éšè—å±‚æ•°")
    parser.add_argument("--data-chunk-length", type=int, default=8, help="RNN è®­ç»ƒæ—¶çš„æ•°æ®å—é•¿åº¦")

    return parser


class EnvIDFilter(logging.Filter):
    def __init__(self, env_id):
        super().__init__()
        self.env_id = env_id

    def filter(self, record):
        record.env_id = f"{self.env_id}"
        return True


def setup_logging(env_id=0, log_file=None):
    """é…ç½® loggingï¼Œè®©æ—¥å¿—æ—¢è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œåˆå†™å…¥æ–‡ä»¶ï¼Œæ ‡æ˜ ENV ID"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    # åˆ›å»º Filterï¼Œç”¨äºæ³¨å…¥ env_id
    env_filter = EnvIDFilter(env_id)

    # æ—¥å¿—æ ¼å¼å¸¦ env_id
    formatter = logging.Formatter("%(asctime)s - %(levelname)s [ENV %(env_id)s] - %(message)s")

    # ç»ˆç«¯ handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    console_handler.addFilter(env_filter)

    # æ–‡ä»¶ handler
    file_handler = logging.FileHandler(log_file, mode="a")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    file_handler.addFilter(env_filter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    logging.info(f"Logger for ENV {env_id} initialized, log path: {log_file}")


# ========== 6. è®­ç»ƒ PPO ==========
if __name__ == "__main__":
    num_envs = 16    # è®¾å®š 8 ä¸ªå¹¶è¡Œç¯å¢ƒï¼ˆæ ¹æ® GPU æ€§èƒ½è°ƒæ•´ï¼‰

    log_file = "./train/result/train_dodge4.log"

    # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
    def make_env(env_id):
        setup_logging(env_id, log_file)
        return SB3SingleCombatEnv(env_id, config_name='1v1/DodgeMissile/HierarchyVsBaselineSelf')


    # env = SubprocVecEnv([lambda: make_env(env_id) for env_id in range(num_envs)])
    env = SubprocVecEnv([lambda env_id=env_id: make_env(env_id) for env_id in range(num_envs)])

    # å®šä¹‰ PPO æ¨¡å‹ï¼ˆè‡ªå®šä¹‰ MLP ä½œä¸ºç‰¹å¾æå–å™¨ï¼‰
    policy_kwargs = dict(
        features_extractor_class=CustomPolicy,
        features_extractor_kwargs=dict(action_dim=env.action_space)
    )

    # æ¨¡å‹è·¯å¾„
    model_path = "./trained_model/dodge_missile/ppo_air_combat_dodge2.zip"

    if os.path.exists(model_path):
        print("âœ… åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ...")
        model = PPO.load(
            model_path,
            env=env,
            tensorboard_log="./ppo_air_combat_tb/dodge4/",
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
    else:
        print("ğŸ†• æ²¡æœ‰æ—§æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒä¸€ä¸ªæ–°çš„ PPO æ¨¡å‹")
        # åˆ›å»º PPO æ¨¡å‹
        model = PPO(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.02,
            verbose=1,
            tensorboard_log="./ppo_air_combat_tb/dodge3/",
            device="cuda" if torch.cuda.is_available() else "cpu"
        )

    # åˆ›å»º checkpoint å›è°ƒï¼Œæ¯ 10 ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡
    checkpoint_callback = CheckpointCallback(
        save_freq=10_000,  # æ¯ 1*num_env ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡
        save_path="./trained_model/dodge_missile_checkpoints4/",  # ä¿å­˜æ–‡ä»¶å¤¹
        name_prefix="ppo_air_combat_dodge"  # æ–‡ä»¶åå‰ç¼€
    )

    # å¼€å§‹è®­ç»ƒï¼ŒåŒæ—¶è®°å½• TensorBoard å’Œä¿å­˜ä¸­é—´æ¨¡å‹
    model.learn(
        total_timesteps=3_000_000,
        tb_log_name="test_dodge4",
        callback=checkpoint_callback
    )

    # æœ€ç»ˆè®­ç»ƒå®Œæˆåä¿å­˜ä¸€æ¬¡å®Œæ•´æ¨¡å‹
    model.save("./trained_model/dodge_missile/ppo_air_combat_dodge4")

    # å…³é—­ç¯å¢ƒ
    env.close()
